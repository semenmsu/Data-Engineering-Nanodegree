## Project Redshift Data Warehouse

### Project summary

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in Amazon S3 storage, in directories containing JSON logs on user activity and metadata on songs.

In this project we are going to create an ETL pipeline for moving meta and log data to Redshift cluster and increase speed for query analysis for analytics team.

### S3 Datasets

- Song Dataset
  - Each file is in JSON format and contains metadata about a song and the artist of that song.
- Event Log Dataset
  - The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above.

### Redshift Cluster

Using the song and event datasets,we are going to create a star schema optimized for queries on song play analysis. This includes the following tables.

- Fact Table
  - songplays (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
- Dimension Tables
  - users (user_id, first_name, last_name, gender, level)
  - songs (song_id, title, artist_id, year, duration)
  - artists (artist_id, name, location, lattitude, longitude)
  - time (start_time, hour, day, week, month, year, weekday)

### ETL

- drop and create needed tables (drop for idempotency)
- move from S3 to Redshift staging tables (using COPY)
- create start schema from staging tables for analytics team

### Conifg

You need to configure the settings in the file dwh.cfg. Create role with access to S3 and inpute correct access credentials for Redshift cluster. Be careful with this information. Don't make it public.

### How To Run

- setup dwh.cfg
- run create_tables.py
- run etl.py

### Helpful scripts

- aws s3 ls s3://udacity-dend/log_data/ - test data set log_data
- aws s3 ls s3://udacity-dend/song_data/ - test data set song_data
- aws s3 cp s3://udacity-dend/log_json_path.json . - download schema
